Project Title: BugScout AI
Project Summary: 
BugScout AI is a self-learning, multi-agent LLM system that detects
real-time UX friction in production, maps it to your codebase, and delivers governed,
continuously improving fixes under human oversight.
System Overview:
BugScout AI is a self-learning production monitoring system that automatically detects,
diagnoses, and suggests code-level fixes for web application issues by analyzing
real-time PostHog session replays. The system employs a coordinated four-agent LLM
architecture: Issue Monitoring Agent, Codebase Crawler Agent, Solution Agent, and
Self-Learning Agent, that work together to transform raw user session data (exceptions,
rage clicks, dead clicks, console errors) into precise, actionable fixes with correct file
paths and code snippets. Unlike generic LLMs, BugScout AI learns continuously from
developer feedback, building category-specific knowledge summaries that enables
improvement in code location accuracy. The entire pipeline operates on private
production data from multiple domains (academic platforms, industrial software,
enterprise SaaS, telecommunications), stored in NeonDB for integrity and ChromaDB
for semantic retrieval, creating a unique dataset that baseline models cannot access.
Pipeline:
1. Real-Time Data Ingestion: 
BugScout AI starts by collecting website logs, live session traces, API errors, and
system events in real time. Data is collected in two primary ways:
a. Live production data from partner company websites (real user interactions, not
synthetic test data)
b. Private institutional datasets securely provided by companies across multiple
domains, including academic platforms, industrial automation systems, enterprise
SaaS products, and telecommunications infrastructure
This gives BugScout AI access to multi-domain, real-world production environments,
making the dataset highly novel and difficult to replicate.
This level of data novelty and access difficulty creates a competitive advantage, as
baseline models and public systems cannot replicate or access similar private,
production-grade session streams.
2. Data Cleaning & Normalization:
Raw logs often contain noise, duplicates, and inconsistent formats. The preprocessing
layer cleans, deduplicates, and normalizes this data.
This step ensures that anomaly detection, classification, and summarization operate on
high-quality inputs, significantly improving model performance and reducing false
positives.
3. Storage & Intelligent Indexing Layer
BugScout AI uses a dual storage architecture:
a. Structured Database (NeonDB) for categorized logs and metadata
b. Vector Database (ChromaDB) for semantic embeddings and contextual search
Logs are automatically embedded and indexed, enabling both keyword-based retrieval
and semantic reasoning. This powers retrieval-augmented generation (RAG) for
accurate solution generation.
4. Multi-Agent AI Orchestration
BugScout AI operates through specialized AI agents, each responsible for a specific
task:
a. Issue Classification Agent: Categorizes errors into predefined or dynamically
generated issue types
b. Issue Monitoring Agent: Detects recurring patterns, anomaly spikes, and
confidence trends
c. Codebase Crawler Agent: Maps detected issues to relevant files and code
dependencies
d. Solution Agent: Generates context-aware fixes using retrieved documentation,
past fixes, and embeddings
This modular agent-based architecture enables scalability, explainability, and parallel
processing.
5. Self-Correcting LLM Engine (Core Intelligence Layer)
At the heart of BugScout AI lies the Self-Correcting LLM.
Unlike static models, this LLM:
a. Processes logs to create dynamic category summaries
b. Identifies confidence trends in predictions
c. Cross-verifies generated solutions against retrieved context
d. Refines outputs based on historical approvals
The model continuously evaluates its own responses and improves through structured
feedback, reducing hallucinations and increasing precision over time.
6. Human-in-the-Loop Validation
Proposed fixes generated by the Solution Agent are sent to developers for structured
review. They can approve, rate, modify, or reject AI-generated outputs before
deployment.
This ensures full human oversight, transparency, and traceability of AI decisions. All
actions are logged, and developers retain final authority over system outputs.
The pipeline is designed in alignment with the EU AI Act, particularly emphasizing
human oversight, accountability, and risk mitigation through controlled validation and
audit mechanisms.
7. Continuous Learning & Feedback Loop
Approved fixes and developer feedback are stored back into the knowledge base.
The system updates:
a. Category summaries
b. Retrieval relevance
c. Confidence scoring models
d. File-location mappings
e. This creates a closed-loop improvement cycle, enabling BugScout AI to become more
accurate, faster, and context-aware with every validated fix.

Models and Tools:
LLM:
Primary Model: 
OpenAI GPT-4o-mini. Used for all four agents: Issue Monitoring Agent,
Solution Agent, Self-Learning Agent, and Codebase Crawler Agent. Chosen for
cost-effectiveness, speed, and strong reasoning capabilities. JSON mode enabled for
structured outputs. Max tokens: 2000 (Issue Monitoring), 2500 (Solution Agent), 3000
(Self-Learning Agent), 2000 (Codebase Crawler Agent).
Embeddings:
Model: 
OpenAI text-embedding-3-small (1536-dimensional embeddings). Batch
processing: 100 documents per batch for efficiency. Used for all vectorization:
monitoring data, issues, logs, PostHog events.
Retrieval:
Vector Database: 
ChromaDB Cloud. Collections: monitoring, issues, logs,
posthog_events. Semantic similarity search using cosine distance. Metadata filtering by
severity, table type, recording ID. Automatic embedding generation via custom OpenAI
embedding function.
Reranking:
Hybrid Approach: 
Recency-based ranking (most recent logs filtered by category
prioritized, 25 entries), developer rating weighting (high-rated solutions 4-5 boost agent
confidence scores), semantic similarity (vector search finds contextually similar issues
even without exact matches), codebase map alignment (file path matching improves
code location accuracy).
Hosting/UI:
Frontend: 
Next.js 14 with React, TypeScript, Tailwind CSS.
Backend: 
Next.js API Routes (serverless functions)
Database: 
Neon PostgreSQL (serverless Postgres); Vector Store: ChromaDB Cloud.
Authentication: 
Clerk. Analytics: PostHog (session replays, events, feature flags)
Deployment: 
Vercel with cron jobs for auto-sync.
Why this works?:
1. Multi-Agent Architecture:
Four specialized agents working in coordination: Issue Monitoring Agent (detection and
classification), Solution Agent (fix generation with context from other agents),
Self-Learning Agent (knowledge summarization by category), Codebase Crawler Agent
(dynamic codebase analysis). Each agent receives specialized context, reducing
cognitive load and improving accuracy. Allows independent optimization of each stage.
Self-Learning Agent enables faster context retrieval through category-based
summarization.
2. Real-Time Data Processing:
Direct integration with PostHog provides live session data. Issues are detected as they
occur, not in retrospect. Enables proactive issue resolution before widespread user
impact.
3. Self-Learning Knowledge Base:
Developer ratings create a feedback loop that improves over time. Self-Learning Agent
processes all approved fixes and creates category-specific summaries. Category
summaries enable faster context retrieval for the Solution Agent. Vector similarity
search enables retrieval of relevant past solutions even with slight variations. High-rated
solutions inform confidence scoring, making the system more reliable. Summarization
reduces token usage while preserving critical knowledge patterns.
4. Codebase-Aware Context:
CODEBASE_MAP.json provides static file structure and component roles for smaller
codebases. Codebase Crawler Agent dynamically analyzes large codebases (1000+
files) in real-time. Agents can pinpoint exact code locations and suggest file-specific
fixes. Reduces hallucination and improves actionable suggestions. Adapts to codebase
changes without manual map updates.
5. Structured Data Pipeline:
NeonDB ensures data integrity and ACID compliance. ChromaDB enables fast
semantic search at scale. Automatic synchronization keeps both stores in sync.
Deduplication prevents redundant processing.
6. Multi-Signal Analysis:
Combines multiple signals: exceptions, rage clicks, dead clicks, console errors, page
events. Pattern recognition across sessions identifies recurring issues. Severity
classification considers impact, not just error presence.
Demonstrated Improvement:
Baselines:
Baseline 1:
Manual issue detection (human review of PostHog session replays)
Baseline 2:
Rule-based detection (threshold-based alerts for error counts, rage clicks)
Baseline 3:
Generic LLM without context (GPT-4o-mini with only session data, no
codebase map or past solutions)
Method:
Test Dataset:
50 real session recordings from production PostHog data
Evaluation Metrics:
Issue Detection (rate of actual issues correctly identified), False
Positive Rate (rate of non-issues flagged as issues), Code Location Accuracy (% of
issues with correct file path identification), Fix Quality Score (developer rating 1-5 for
suggested fixes), Time to Resolution (time from issue detection to fix suggestion),
Confidence Score Accuracy (correlation between agent confidence and developer
rating)
Metric:
Primary:
Developer Rating (1-5 scale for suggested fixes)
Secondary: Issue Detection Precision/Recall, Code Location Accuracy, Confidence
Score Correlation
